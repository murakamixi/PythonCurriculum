機械学習とは
# この章の目標
　機械学習とは何かを理解します。
　また、機械学習の種類の中でも教師あり学習とはどのような流れで行うのか？ということを理解することです。

**推定完了時間:**

1. 教師あり学習とは？

　Pythonでの基本的な文法はすでにマスターしたと思います。そこで、今回はpythonを使ってデータを分析し、未来を予測する機械学習を行なっていきます。

# 概要
　今回は、機械学習の種類とその中の教師あり学習について学習しましょう。また、機械学習はどのような流れで開発が行われるのか？その全体像についても解説していきたいと思います。

# 機械学習とは
　**機械学習とは、大量のデータを学習し、分類や予測などを行うアルゴリズムやモデルを自動的に構築するもの**です。現在の人工知能（AI）の中心的な技術であり、ディープラーニングやニューラルネットワークも機械学習の一部です。

### IMG（DLなどの関係図）

　また、機械学習には大きく分けて以下の3種類があります。

1. **教師あり学習**
2. **教師なし学習**
3. **強化学習**

それぞれ、メリットデメリットが存在しますが、ここではどのようなものなのかということをお伝えできればと思います。


### 教師あり学習
　教師あり学習とは、**正解データが与えられている機械学習の学習方法**になります。教師あり学習は未知のデータに対する予測正解データが与えられたものからの予測を行います。

### 教師なし学習
　大量のデータを与え、アルゴリズム自身がそのデータを探索することで、データの構造やパターンなどを抽出したり、データを分類し、そのデータからの特徴を探し出します。「正解」データは与えられません。

### 強化学習
　強化学習」は、教師なし学習と同じく「正解」データは与えられませんが、データの出力を価値づけし、その価値を最大化するための行動をとるようにアルゴリズムを最適化します。現在では、AlphaGoに代表されるようなゲームに多く利用されています。

## 教師なし学習のメリットとデメリット
　今回は教師なし学習をメインに取り上げていきます。そこで、教師あり学習のメリットとデメリットについて詳しくみていきましょう。

### 教師あり学習のメリット

**人間が正解を教えるため性能が高い**

　教師あり学習はあらかじめデータの中に<font color='red'>**目的変数と呼ばれる正解のデータ**</font>が存在します。ですので、あらかじめデータセットの中に答えが用意されています。また、一般にほかの学習よりも早いと言われています。

### 教師あり学習のデメリット

**正解がない分野では利用できない**

　「ある購入者にこちらもみんな買っていますよ」というおすすめ機能は見たことはないでしょうか？こちらは、教師なし学習を用いた代表的な例です。この機能は、常に正解がなくいため教師あり学習は用いることができません。

**教師データの質が学習を左右してしまう**

　教師データの答えがそもそも間違っていたらどうでしょうか？正しい答えが出るはずがありませんね。ですので、出来るだけ正しいと思われるデータを集めることが重要なのです。

# 機械学習の流れ
　機械学習の一連の流れをここでは説明します。
　ゴールの設定（決定）の上で機械学習プロジェクトは以下のようなフローで進められます。ただし、一発で上からしてへうまく降りていくというよりはその工程でダメだったらまた戻るというようなことを繰り返します。

1. **データを収集する**
2. **データを理解する＆データの前処理**
(データクレンジングとも呼ばれ、データの重複やあるべきデータがない欠損データなどを取り除いてデータの精度を高めること。)
3. **特徴量、教師データの設計**
4. **機械学習アルゴリズムの選択と学習(機械学習)**
5. **テストデータを使った性能評価**
6. **ハイパーパラメーターチューニング**
7. **サービスに組み込む**

　上記フローの中でも、最も時間のかかる部分はどこでしょうか？
　それは、データを集めることや、データを使える形に前処理するといった工程です。これは、実際のデータは入っているべきデータが入っていなかったりします。それを整える作業のことです。
　また、実際に機械学習を行っているのは4番に当たります。
　機械学習を行う際には、データを例えばWebから集めたり、社内で作成したり、データを買ったりするという工程は地道ですがとても大事だということをここでは理解してください。

　以下は機械学習プロジェクトの手順を図で示したものです。

### IMG(developedフロー)

# まとめ

**------------------------------**

今回使うデータの確認
# この章の目標
　scikit-learnのデータの内容について学習します。また、今回学習するデータについて中身をみてみましょう。

1. scikit-learnのボストンの住宅価格データについて理解する

# 概要

　scikit-learnの付属データである**トイデータセット**を使います。トイデータセットというのは、通常の機械学習で使うものと比べるとデータ数が少ないデータセットです。
　ですが、学習に使うのにはインストールして付属してくるので使い勝手が良いいので今回こちらのデータセットのうち、**ボストンの住宅価格データを使います**。そのデータについて理解しましょう。

# scikit-learnとは

  scikit-learn（サイキット・ラーン）はPython用の機械学習ライブラリです。scikit-learnはオープンソースで公開されており、誰でも無料で利用することが出来ます。また、教師あり学習、教師なし学習に関するアルゴリズムが一通り利用できます。このなかに今回使う単回帰分析、重回帰分析、多項式回帰が含まれています。

## scikit-learnのデータセット

　scikit-learnには、付属データとして**トイデータセット**が付属しています。トイデータセットというのは、通常の機械学習で使うものと比べるとデータ数が少ないデータセットです。ですが、学習に使うのにはインストールして付属してくるので使い勝手がいいので今回はこちらを使います。
　また、トイデータセットの中でも、ボストンの住宅価格のデータセットを使います。
　ほかのデータセットは以下の表に記載されています。

|データセット名|呼び出し方|
|:--:|:--:|
|ボストンの住宅価格|load_boston()|
|アイリス（アヤメ）の種類|load_iris()|
|糖尿病の進行状況|load_diabetes()|
|手書き文字（数字）|load_digits|
|生理学的測定結果と運動測定結果|load_linnerud()|
|ワインの種類|load_wine()|
|がんの診断結果|load_breast_cancer()|

# ボストンの住宅価格のデータセットについて知ろう

　ボストンの住宅価格のデータセットのデータをGoogle Colabで読み込んで実際に見てみましょう。

## データを知るためには何が必要でしょうか？

　データの概要を把握することはデータサイエンティストやAIエンジニアにとって大切です。では、データを見るときに何をみたらそのデータの大まかな内容が把握できるでしょうか？考えてみましょう。
　データを分析の指標はたくさんあります。
　例えば、まずデータを表にした時の項目名やその行数と列の数、これはどれくらいのデータの大きさなのか、どんなデータなのかの指標になります。
　次に、最大値最小値、平均値などを求める方法があります。これは、そのデータの広がり具合を示す指標によく使われます。皆さんですと箱ひげ図が馴染み深のではないでしょうか？
　それでは１つづつみていき、データの概要を掴んでみましょう。

## データセットを読み込む

  <font color='red'>**import**</font>文で読み込んでみましょう。

```python
from sklearn.datasets import load_boston
```

## 読み込んだデータセットを表示して詳細をみよう

  これだけで実行しても何も表示されません。それはそうですね、読み込んでいるだけですから。そこで、トイデータの中身を表示する<font color='red'>**DESCR**</font>というメソッドを使ってload_bostonの詳細データをprintしてみましょう。
  

  ### どんなデータなのかみてみよう
  以下のコードを実行してみてください。そうすると、データを表にした時の項目名などを見ることができます。

  ```python
  dataset = load_boston()
  # dataset.DESCR を表示すると、データセットの概要が表示されます。
  print(dataset.DESCR)
  ```

  すると、以下のようにデータセットの詳細が表示されたはずです。

  ```python
  Boston house prices dataset
  ---------------------------

  **Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
      - CRIM     per capita crime rate by town
      - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
      - INDUS    proportion of non-retail business acres per town
      - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
      - NOX      nitric oxides concentration (parts per 10 million)
      - RM       average number of rooms per dwelling
      - AGE      proportion of owner-occupied units built prior to 1940
      - DIS      weighted distances to five Boston employment centres
      - RAD      index of accessibility to radial highways
      - TAX      full-value property-tax rate per $10,000
      - PTRATIO  pupil-teacher ratio by town
      - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
      - LSTAT    % lower status of the population
      - MEDV     Median value of owner-occupied homes in $1000's
  ```

英語のままで難しいと思いますのでいかに日本語訳を用意しました。参考までにどうぞ。

|英語カラム名|日本語カラム名|
|:--:|:--:|
|CRIM|1人当たりの犯罪数|
|ZN|町別の25,000平方フィート(7600m2)以上の住居区画の割合|
|INDUS|町別の非小売業が占める土地面積の割合|
|CHAS|チャールズ川沿いかどうか|
|NOX|町別の窒素酸化物の濃度（1000万分の1）|
|RM|住居の平均部屋数|
|AGE|持ち家住宅|
|DIS|5つのボストン雇用センターへの重み付き距離|
|TAX|町別の$10,000ドルあたりの固定資産税率|
|PTRATIO|町別の生徒と先生の比率|
|TRACT|土地番号|
|B|1000*(黒人人口割合 - 0.63)2割合が低いとスコアが高くなるようになっている。|
|LSTAT|貧困人口割合|
|MEDV|持家住宅の価格(1000USD単位)|

　これらは、カラム名と呼ばれます。表の項目名に当たるような部分です。

### データの大きさ
次に、何行何列のデータなのかを上の結果からみてみましょう。

```
:Number of Instances: 506 
:Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.
```

の部分では、「データの行数は506あり列数は14列ありますよ」「要素の14番目が多くの場合は目的変数で使われるよ」と教えてくれています。英語に臆することなく前に進みましょう。




### IMG(カラム名の写真)


### もっとデータの概要を把握しよう

　次に、最大値最小値、平均値などを求めるますが、scikit-learnだけで求められなくもないですが、もっと簡単に求めるモジュールがありますので次章でご紹介したいと思います。まずは、今回の章の復習をしっかりしていきましょう。



# まとめ

**------------------------------**

データを均一化しよう
# この章の目標

　pandasを使ってデータの前処理の方法を習得しましょう。また、pandasはどのような時に使われるかも理解しましょう。

1. pandasの使い方と現実のデータを理解する

# 概要
　ここでは、機械学習の大半の時間をかけるであろうデータの前処理に関して学習します。データの前処理を効率化するためにpandasというモジュールについても学習します。また、pandasで最大値最小値、平均値などを求めてみましょう。

# pandasとは

　Pandas(パンダス)はデータ解析を容易にする機能を提供するPythonのデータ解析モジュールです。
Pandasの特徴には、データフレーム(DataFrame)などの独自のデータ構造が提供されており、様々な処理を高速に実行することが可能です。
　pythonではpandasを使ってデータの精査や前処理を行うのが一般的です。

## pandasのデータ構造
　Pandasでは、データ構造をそのデータが1次元か2次元かで分かれています。

|次元|表し方|
|:--:|:--:|
|1次元|Series|
|2次元|DataFrame|
|3次元|Panel|

　ですが、1次元と2次元おもに注目していきます。SeriesとDataFrameは名前は違いますが、1次元と2次元をいいたりきたりするために切り離して考えることはできません。
　このデータ構造を理解することはPandasを理解する上で非常に重要ですので、理解するようにしましょう。

### Series
　Seriesは1次元データを保存します。実際にボストンの住宅価格のデータ1行ずつをSeriesに入れてみましょう。

　まず、ボストンの住宅価格のデータの1行目をみてみましょう。データの取り出し方がわからない場合は基本編の復習をしましょう！

```python
from sklearn.datasets import load_boston
import pandas as pd

dataset = load_boston()
line = dataset.data[0]

print(line)
```

　これで、横一列のリストが表示されたと思います。
　これをSeries型に型変換してSeries型と単なるリスト型との違いをみていきましょう。

```python
from sklearn.datasets import load_boston
import pandas as pd

dataset = load_boston()
line = pd.Series(dataset.data[0])

print(line)

# 出力結果
0       0.00632
1      18.00000
2       2.31000
3       0.00000
4       0.53800
5       6.57500
6      65.20000
7       4.09000
8       1.00000
9     296.00000
10     15.30000
11    396.90000
12      4.98000
dtype: float64
```

このように、Series型では横にcolumn名がついた状態で表示されます。ですが、このままでその数字すなわち行がどんな意味を持っているかわかりませんね。ですので、この行に名前をつけるために<font color='red'>**index**</font>というSeriesの引数を使ってみましょう。また、名前は、ボストンの住宅価格のデータセットを見ると**feature_names**というkeyで保存されていることがわかります。

```python
from sklearn.datasets import load_boston
import pandas as pd

dataset = load_boston()
line = pd.Series(dataset.data[0], index=dataset.feature_names)

print(line)

# 出力結果
CRIM         0.00632
ZN          18.00000
INDUS        2.31000
CHAS         0.00000
NOX          0.53800
RM           6.57500
AGE         65.20000
DIS          4.09000
RAD          1.00000
TAX        296.00000
PTRATIO     15.30000
B          396.90000
LSTAT        4.98000
dtype: float64
```
　
　これで、どのデータがどんな意味を持っているのか意味がわかるようになりましたね。ですが、ボストンの住宅価格のデータセットは502行と14列を持つデータです。これを全て表すのはどうやら2次元データとして扱った方が良さそうですね。次は2次元データであるDataFrameをみていきましょう。

### DataFrame
　DataFrameは2次元データを保存します。実際にボストンの住宅価格のデータをDataFrameに入れて全てのデータを扱えるようになりましょう。

　まず、ボストンの住宅価格のデータをDataFrameに変換してみましょう。

```python
from sklearn.datasets import load_boston
import pandas as pd

dataset = load_boston()
data = pd.DataFrame(dataset.data)

print(ldata)

# 出力結果
          0     1      2    3      4   ...   8      9     10      11    12
0    0.00632  18.0   2.31  0.0  0.538  ...  1.0  296.0  15.3  396.90  4.98
1    0.02731   0.0   7.07  0.0  0.469  ...  2.0  242.0  17.8  396.90  9.14
2    0.02729   0.0   7.07  0.0  0.469  ...  2.0  242.0  17.8  392.83  4.03
3    0.03237   0.0   2.18  0.0  0.458  ...  3.0  222.0  18.7  394.63  2.94
4    0.06905   0.0   2.18  0.0  0.458  ...  3.0  222.0  18.7  396.90  5.33
..       ...   ...    ...  ...    ...  ...  ...    ...   ...     ...   ...
501  0.06263   0.0  11.93  0.0  0.573  ...  1.0  273.0  21.0  391.99  9.67
502  0.04527   0.0  11.93  0.0  0.573  ...  1.0  273.0  21.0  396.90  9.08
503  0.06076   0.0  11.93  0.0  0.573  ...  1.0  273.0  21.0  396.90  5.64
504  0.10959   0.0  11.93  0.0  0.573  ...  1.0  273.0  21.0  393.45  6.48
505  0.04741   0.0  11.93  0.0  0.573  ...  1.0  273.0  21.0  396.90  7.88
```

  このように、DataFrame型では横にIndexがついた状態、縦にカラム名がついた状態で表示されます。ですが、このままでその縦の数字すなわち列がどんな意味を持っているかわかりませんね。ですので、この列それぞれに名前をつけるために<font color='red'>**columns**</font>というDataFrameの引数を使ってみましょう。また、Seriesの時と同様に名前は、ボストンの住宅価格のデータセットを見ると**feature_names**というkeyで保存されていることがわかります。

```python
from sklearn.datasets import load_boston
import pandas as pd

dataset = load_boston()
data = pd.DataFrame(dataset.data, columns=dataset.feature_names)
print(data)

# 出力結果
        CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT
0    0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98
1    0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14
2    0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03
3    0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94
4    0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33
..       ...   ...    ...   ...    ...  ...  ...    ...      ...     ...    ...
501  0.06263   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  391.99   9.67
502  0.04527   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   9.08
503  0.06076   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   5.64
504  0.10959   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  393.45   6.48
505  0.04741   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   7.88
```
　
　これで、どの列データがどんな意味を持っているのか意味がわかるようになりましたね。

## DataFrame型の便利なメソッドを使ってみよう

　DataFrame型には多くの便利なメソッドが用意されています。その１つである<font color='red'>**describe()**</font>メソッドを使ってデータの平均値や最大値、最小値をみてみましょう。

### <font color='blue'>describe()メソッドを使ってデータの特徴をみてみよう</font>

```python
data = pd.DataFrame(dataset.data, columns=dataset.feature_names)
data.describe()
```


<!-- ですが、これはあらかじめ成形されたデータであり、仮にデータが空の値をすなわち空白を含んでいたら、違う型のデータが入っていたらどうでしょうか？ -->
<!-- 　余力があれば以下の節を読んで実際のデータの処理の仕方を学習しましょう。
　実際のデータは空白のデータが入ってることは日常茶飯事です。ですので空白のデータの探し方やそのカラムのデータ型を見る方法を学習しましょう。 -->

<!-- # データの前処理の流れを確認しよう

機械学習プロジェクトにおいて７〜８割は前処理に時間が費やされるといわれています。
まず最初に、機械学習における文字列の扱い方について説明します。
機械学習において多くの場合は生のデータは役に立たず、意味のある数値に変換できて初めて様々な機械学習アルゴリズムに入力することができます。
そのためには、生データから取得した文字列や数値、日付データ等を、何らかの数値で表した「特徴量」にしないといけません。
### データの詳細情報を確認をする
### データに空白がないだろうか？ -->

**------------------------------**

住宅価格に最も影響ある変数を見つけよう
# この章の目標

　今は、ボストンの住宅価格のデータセットの住宅価格を求めるモデルを作りたいのですね。その住宅価格を一番左右する変数はなんなのかデータ間の関係を求めて考える方法を学習しましょう。

1. データ間の相関に関して考えられるようになる

# 概要
　pandasを使ってデータ間の相関係数を求めましょう。相関係数がどのような場合に目的の変数を多く動かすようなような要因になっているのでしょうか？学習してみましょう。

# 目的変数と説明変数
　今回行うことを再確認しましょう。ボストンの住宅価格のデータセットを分析して、目的変数である住宅価格を求めるための直線を引くことが目的でしたね。ですのでそこで改めて目的変数と説明変数を少し掘り下げて解説したいと思います。
### 目的変数
　目的変数は、住宅価格のデータです。
　おもに目的変数とは、「結果となっている変数」のことを言います。住宅価格というのは場所の特性やその人の年収などによって変動することが考えられますよね？ですので、そのような場所やその人の年収の結果が目的変数「住宅価格」として今求めるものになっているのです。
　ちなみに、ボストンの住宅価格のデータセットでは以下のようにして目的変数を取り出せます。

```python
from sklearn.datasets import load_boston

dataset = load_boston()
y = dataset.target

print(y)
```

### 説明変数
　説明変数は、住宅価格を左右する変数のことです。
　住宅価格は先ほどの例を使うと場所の特性やその人の年収などによって変動することが考えられます。その考えらえれる要素、すなわち目的変数を生む原因を説明変数というのです。
　ですので、目的変数と説明変数は「説明変数を受けて発生した結果となっている変数」ということができます。
　ちなみに、ボストンの住宅価格のデータセットでは以下のようにして説明変数を取り出せます。

```python
from sklearn.datasets import load_boston

dataset = load_boston()
x = dataset.data

print(x)
```

IMG（説明変数と目的変数）

# データ間の関係を見るにはどうしたらいいのか？
　それでは、目的変数と説明変数は理解しましたね。住宅価格は先ほどの例を使うと場所の特性やその人の年収などによって変動することが考えられますが、どの説明変数がどの程度目的変数に影響しているのでしょうか？例えば一番年収が住宅価格に影響しているとしたともいます。ですが、あるひとは場所だろうと言ったとします。人間がこう思うということで議論していてはキリがありません。ですので、<font color='red'>**相関係数**</font>というデータ間の関係を示す数学の知識を用いてどの説明変数がどの程度目的変数に影響しているのかみてみましょう。

## 相関係数とは
　数学で、相関があることを

- **一般に片方が増えると他方も増えるような関係を正の相関がある**
- **一般に片方が増えると他方も減るような関係を負の相関がある**

というよう言うのは習ったかと思います。相関係数で何が判断できると思いますか？相関係数ではデータの単位や種類に寄らず正負の相関関係や相関の強さ（その程度データ間強い関係を持っているか）を持っているかを判断できるようになります。
　ですので、まず、相関係数を求めてみましょう。

IMG(相関係数の求め方)

$$相関係数 = \frac{\frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^2}
\sqrt{\frac{1}{n} \sum_{i=1}^{n}(y_i - \bar{y})^2}
}$$

|||
|:--:|:--:|
|$n$|データ総数|
|$x_i$|i番目xのデータの値|
|$y_i$|i番目yのデータの値|
|$\bar{x}$|xの平均点|
|$\bar{y}$|yの平均点|


手計算で解くとこんな感じだったではないでしょうか？こんなことをやっていては途方もない時間がかかってしまいすのでもっとプログラムで求めていきましょう。

### 相関係数の求め方を知ろう

　相関係数をpandasを使って求めてみましょう。今回求める関係は各説明変数と目的変数の間の関係を求めます。まずは、下準備として、目的変数と説明変数がバラバラになっていますのでデータを合体させましょう。

IMG（合体イメージノズ）

```python
# データフレーム型に変換
df_x = pd.DataFrame(dataset.data, columns=dataset.feature_names)
df_y = pd.DataFrame(dataset.target, columns=["target"])
dfa = pd.concat([df_x, df_y], axis = 1)
```
　ここで出てきた<font color='red'>**axis**</font>と言う引数は縦方向か横方向かどちらに結合させるかを表したもので、列に沿った処理はaxis=0、行に沿った処理はaxis=1と言う風に決められています。

## pandasで相関係数を求めてみよう

　では、下準備も終わりましたので早速pandasを用いて相関係数を求めていきしょう。これは非常に簡単で、相関係数を求めたいデータに対して、<font color='red'>**corr**</font>メソッドを使うだけでできます。では実際に使ってみましょう。

```python
dfa = pd.concat([df_x, df_y], axis = 1)
# 説明変数と目的変数（target）の関係を見るための各相関関係数を出してみます。
corr = dfa.corr()
corr
```

　相関係数が以下のように出力されていませんか？

IMG(相関係数出力されて際の写真)

　これでどの変数と変数の関係がどの程度の強さなのか数式で表すことができ、みんなが納得する数字となりました。ですが、このまま一個一個見て行っていては今は少ないのでそんない手間ではないでしょうがこれが100変数あったら大変ではないでしょうか？ですので、強さごとに色をつけて見てみることにしましょう。

### 色をつけてみやすくしよう

　先ほどの相関係数の表に色をつけてデータを見やすくしてみましょう。今から以下のような図を作っていきます。飛躍的に見やすくなっていることがわかるでしょう。

IMG（今から作るず）

　その際に使うのが、seabornとmatplotlibです。matplotlibに関しては「回帰分析の結果をグラフに表示しよう」で詳しく説明しますので、ここではこう書けばこう動くんだくらいの認識で構いません。
　また、seabornには数多くの表を美しく見せる機能を備えているライブラリーです。簡単に言えば、seabornはグラフを美しかけるけどもできることが少なく、matplotlibはたくさんのグラフに対応しているけども美しくはないことが多いと言うことです。

IMG（seabornとmatplotlibの違い）

### 実際にseabornを使ってみましょう

　今回はseabornのheatmapという機能を使います。この機能はヒートマップで指定した表を表してくれます。seabornのheatmapの構文は以下の通りです。また、よく使いそうな引数も紹介しておきます。

```python
import seaborn as sns

sns.heatmap(データ, 引数(つけたいオプション分だけ))
```

とするとヒートマップがかけます。ヒートマップの引数について以下の表で説明していきます。ヒートマップの引数は主に、その表に対するオプションとして使われます。他にもたくさんあるので公式サイトを見てみましょう。
[seabornのheatmap公式サイト](https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap)

|引数|できること|
|:--:|:--|
|vmin|データの最小値を指定してくれます|
|vmax|データの最大値を指定してくれます|
|center|カラーマップを中央に配置する値を指定します|
|annot|Trueで数値を表示させます、指定しないとFalseとなり、値は表示されません|
|fmt|枠の中の少数に与えるルール|
|cmap|カラーコードを変更できる|

**注意**
　カラーコードを使い際ですが、カラーコードは以下のリンクから選ばなければなりません

　[カラーコード](https://matplotlib.org/examples/color/colormaps_reference.html)

# 実際にseabornを使ってみよう

　ここからは、実際にseabornを使ってみましょう。まず、<font color='red'>**seabornとmatplotlibをimport**</font>しましょう。繰り返しになりますが**matplotlibに関しては今は理解しなくても大丈夫です**。

```python
import seaborn as sns
import matplotlib.pyplot as plt
```

### <font color='blue'>searbornにデータを渡して最大値を1、最小値を−1に指定して、centerは0、小数点は2桁まで表示して、annot=True、 cbar=Trueのヒートマップを作ってみみょう</font>

　演習に入る前に以下のコードmatplotlibのimportを書いて図の大きをを指定してください。でないと小さくてみにくくなってしまします。

```python
plt.subplots(figsize=(12, 10))
```

```python
# 答え
sns.heatmap(dfa.corr(), vmin=-1, vmax=1, center=0, annot=True, fmt=".2f", cbar=True)
```

### 適切な説明変数を１つ選ぼう
　ここまでで、カラー付きのみやすいヒートマップができたと思います。
　このヒートマップをみてみましょう。一目瞭然で<font color='red'>**LSTATが最も住宅価格のTARGETに影響を与えている変数**</font>であることがわかります。
　この、LSTATを使って実際に単回帰分析と言われる手法を学んでいきましょう。

**------------------------------**

回帰分析を学ぼう
# この章の目標
　回帰分析とはなんでしょうか？おそらく、大半の方が初耳でしょう。回帰分析とは、関数をデータに当てはめることによって、ある目的変数の変動を説明変数の変動により説明・予測・影響関係を検討するための手法です。
　それでは、回帰分析の一種である、単回帰分析について学習しましょう。
　回帰分析だけでも専門書になるくらい本来は奥が深いのだという子は心のどこかにとめて入れください。

1. scikit-learnを用いた単回帰分析の手法を学ぶ

# 概要
　単回帰分析の手法とその一般的な考え方を学びましょう。単回帰分析の一般的な考え方は高校数学の範囲でわかるレベルです。緊張せずに進みましょう。

# 単回帰分析とは
　単回帰分析は、目的変数に対して１つの説明変数でその結果を説明しようという分析手法です。直感的に、平面上に直線を引くことができ、
$$y=ax+b (a,bは実数)$$
のように表すことができ単純でわかりやすいのがメリットです。ですが、多くの社会的問題には複雑な要素が噛み合っており用いれないことがほとんどですが、基礎となってる重要な考え方ですし、実験データには使えることが多いです。
　例えば、バネの伸びを図る実験では、

$$y=\frac{1}{2}kx$$

バネの伸びは以上のように示されるため、単回帰分析を用いることができます。
ですので、決して役に立たないものではないので学んでみることにしましょう。

IMG（バネの伸びの画像）

## 次元と種類を統一しよう

　まず、用いるデータの確認をしましょう。データは**目的変数['target']**と**説明変数['LSTAT']**です。ここでまず、それぞれのデータ型を確認してみましょう。

### <font color='blue'>目的変数['target']と説明変数['LSTAT']それぞれのデータ型を確認してみましょう</font>

```python
　data = pd.DataFrame(dataset.data, columns=dataset.feature_names)
```

のdataの中から目的変数の['LSTAT']だけを取り出すにはどうしたら良いでしょうか？調べてみましょう。
また、データ型を確認するには関数**type**メソッドを使えばよかったでしたね！

## データ型の次元と大きさを確認しよう

### <font color='blue'>データ型の次元と大きさを確認する方法を使ってみよう</font>
　データの次元を確認するには**shape**メソッドを使います。実際に使い方を調べて使ってデータの大きさを取得してみましょう。

```python
xdata = data["LSTAT"].values.reshape(-1, 1)
print("変換のデータ型は", type(xdata))
print("変換後の配列の大きさは", xdata.shape)

ydata = dataset.target
print("変換のデータ型は", type(ydata))
print("変換後の配列の大きさは", ydata.shape)
```

reshape(-1,1)についての説明？？？

## scikit-learnを用いて単回帰分析をしてみよう
　scikit-learnで単回帰分析を行うには、<font color='red'>**LinearRegression()モデル**</font>を使います。LinearRegression()関数について知りましょう。

### LinearRegression()を知ろう
　LinearRegression()モデルは線形回帰を行うモデルです。説明されても何がんんだかわからないと思いますので、早速使ってみましょう！
　先ほどみた目的変数targetと説明変数LSTATをLinearRegression()の**fit**を使って単回帰分析します。

　scikit-learnのLinearRegression()モデルをimportします。

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
lin_reg=model.fit(xdata,ydata)
```

「え、これだけなの」と思いませんか？単回帰分析はこれだけで終了なのです。本来の回帰分析の後ろにある理論はそれだけで分厚い専門書になってしまうのでここでは割愛します。
これで、LSTATとTARGETの間の関係性を持った直線を引くことができました。この切片と傾きを表示させてみましょう。表示するためには、LinearRegressionは、interceptで切片、coefで回帰変数（傾き）を求めることができます。

### <font color='blue'>早速LinearRegressionは、interceptで切片、coefで回帰変数（傾き）を求めてみましょう</font>

```python
lin_reg=model.fit(tdata,ydata)
print('切片',lin_reg.intercept_, '\n傾き', lin_reg.coef_)
```

　以下のようなグラフになることを示しており、LSTATの値を入れることで住宅価格を予測できることを示しています。

IMG（グラフで予測できるよ的なやつ）

### グラフにしてみよう
　これだけでは、何が何だかコンピューターや学者にわかっても一般の人にはわかりませんし、学者でない人にもわからないでしょうね。ですのでわかりやすいようにグラフを書いてみましょう。その際にはmatplotlibというのを使います。次はmatplotlibの使い方を学びましょう。

**------------------------------**

回帰分析の結果をグラフに表示しよう
# この章の目標
　単回帰分析はできたでしょうか？
　単回帰分析はできてもそれはどのようなものか見てみたくなりませんか？
　見てくなったらそんな時に書くのは、グラフではありませんか？グラフを書く方法をここでは学びましょう。

1. matplotlibモジュールを用いてグラフを書く方法を学習する

# 概要
　単回帰分析はできたと思いますが、それをグラフに示したほうがわかりやすいでしょう。そんなグラフや図を書く時に便利なmatplotlibを用いる方法を学習しましょう。

# matplotlibとは
　Matplotlib はPythonのグラフ描画のためのモジュールです。Matplotlibを使うことで、グラフの描画やデータの可視化が簡単に行えます。また、扱えるグラフの種類については公式ギャラリーにて実際の例を見ることができます。
[公式ギャラリー](https://matplotlib.org/gallery.html)

## グラフを書いてみよう
　グラフを実際に書いてみましょう。ここでは、線グラフ・分散図の書き方と2つにグラフを１つにする方法を学びましょう。グラフを書くことで分析結果を説明した際にみんな納得するような説明書類が作れるようになるでしょう。
　以下では、**pyplot**モジュールをimportしている前提で進めますので先にimport文を書きましょう。

```python
import matplotlib.pyplot as plt
```

### 線グラフの書き方
　線グラフの書き方は、数量の大きさを表す位置を線で結んだグラフです。時間とともに変化する数量を示すときに利用します。pyplotモジュールの<font color='red'>**plot()**</font>で描画することが出来ます。先ほどの切片と傾きを使ってグラフを書いてみましょう。また、グラフをプロットするために、**predict**という作成した回帰モデルに説明変数の値を与えた時の目的変数（予測）値を得るpredictメソッドを用いています。

　plotの構文は以下の通りです。

```python
plt.plot(プロット範囲, プロットする関数, オプション引数)
```

```python
plt.plot(X_test, model.predict(X_test), color="red")
# グラフを表示するメソッド
plt.show()
```

これで、以下のようなグラフが表示されたのではないでしょうか？

IMG(グラフ)

　ここで、plt.plotにオプションとなる引数がついていますね！
　代表的な引数をみていきましょう。
　またこれでは、なんのグラフかもわからないですし、横軸も縦軸もなんの意味なのかわかりません。ですので、横軸と縦軸やグラフにタイトルなどのつけ方を学びましょう。

### グラフの見た目を変えよう
　ここでは代表的なオプションの引数についてみていきます。これらのオプションを指定しなくても構いませんが指定した方がいい場合もありますし、したくなったときのためにこんなのがあるのかということを知っておきましょう。

|オプション名|項目|表示例|
|:--:|:--:|:--:|
|marker|印|“o”, “v”, “^”, “d”|
|color|線の色|“red”, “blue”, green”|
|linestyle|線の形|“-“, “–“, “-.”, “:”|
|linewidth|線の太さ|1,2,3,4,5|

　また、ラベルとタイトルをつけてみましょう。ラベルとタイトルは以下の表の通りです。参考に先ほどのグラフにラベルをつけてみましょう。

|メソッド名|項目|
|:--:|:--:|
|title|タイトル|
|xlabel|横軸のラベル|
|ylabel|縦軸のラベル|
|legend|凡例を表示|

　以下の例を参考にグラフにラベルをつけてみよう。

```python
plt.title("boston_housing")
plt.xlabel("LSTAT")
plt.ylabel("TARGET")
```

以下のグラフのようになりましたか？

IMG（グラフ）

以上で、回帰分析した結果の直線を見ることができましたが、これでは本当の住宅価格のデータと一致しているかわかりません。ですので、個々の住宅価格のデータのグラフも作成して見比べてみることにしましょう。バラバラのデータを見るときに便利な分散図を使ってみることにしましょう。

### 分散図の書き方
　ここでは分散図の書き方を学習しましょう。分散図は2つの量にどのような関係があるのかを見るときに使うグラフです。分散図は、pyplotモジュールの<font color='red'>**scatter()**</font>で描画することが出来ます。

　scatterの構文は以下の通りです。

```python
scatter(x軸データ,y軸データ,)
```

### <font color='blue'>分散図をx軸に説明変数とy軸に目的変数をとって分散図を表示してみましょう</font>

　これにも線グラフと同様に様々なオプション引数が使えます。線グラフの先ほど紹介したオプションも使えますが、分散図特有のオプション引数が1つあるので紹介します。

|メソッド名|項目|
|:--:|:--:|
|s|プロットのサイズ変更|

　これで1つのグラフに2つのデータを表示することができましたね。
　ところで、そもそも、**「このモデルは本当に正しいのか？」**と疑問に思いませんか？何を持って、このモデル（結果）は正しいと言えるのでしょうか？
　何が正しいのかということを次回学習していきましょう。

**------------------------------**

得られたモデルの性能をテストしよう
# この章の目標

　作成したモデルが本当に未来を予測できるのでしょうか？未来を優れた精度で予測できるモデルかどうなのかそれを証明する手法を学びましょう。

1. scikit-learnを用いて、モデルの正確性を保証する手法を学びましょう。

# 概要
　モデルのテストというモデルの性能評価試験のようなものを行なっていきます。その手法と結果の分析方法を学びましょう。

# モデルのテストについて
　構築したモデルを今後利用することを考えると、モデル構築の際にモデルが得られない将来のデータに対して精度よく予測できるかがそのモデルの性能の重要なポイントです。ですので、学習に用いていないデータでモデルを検証する必要があります。そのため、**手元のデータを学習データと検証データにわけ、学習データでモデルを構築し、検証データを将来のデータと見立て、これに対するモデルの性能を評価します**。この評価方法を実際にやりながら学んでいきましょう。

## データのテストとトレーニングに2分割
　まず、手元のデータを学習データと検証データに分割します。
　分割する方法もたくさんありますが、ホールドアウト法という手法で分割します。他の分割方法に興味があれば調べてみましょう。
　実際に、これをどのようにホールドアウト法で分割するのでしょうか？実は、分割するための関数がscikit-learn内に用意されています。それが、sckit-learnのmodel_selectionのtrain_test_split関数というものです。
　引数には、分割するデータ、学習データと検証データの比率である"test_size"などを指定します。
　random_stateを0にすることでランダムではなく毎回同じ結果を返すことができます。以後に表示する結果は変わってしまいますので指定することをお勧めします。
　いかに構文を示します。

```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(データ, 引数)
```

実際に、ボストンの住宅価格のデータセットを学習データと検証データに分けてみましょう。

```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.20, random_state=0)
```

　これで、学習データと検証データに分割できました。

### <font color='blue'>学習に全データを用いていたので、書き換えて学習には学習データを使うように書き換えましょう。</font>

```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.20,random_state=0)
train.count()
test.count()

# 訓練データとテストデータの作成
X_train = train["LSTAT"].values.reshape(-1,1)
Y_train = train["target"]
X_test = test["LSTAT"].values.reshape(-1,1)
Y_test = test["target"]

lin_reg=model.fit(X_train,Y_train)
```

ちなみに、

```python
train.count()
test.count()
```

で、検証データと学習データの分割数をみることができます。

では、実際にあらゆる数値を用いて学習したモデルの正確性を測っていきましょう。

## 評価に用いる値
　評価に用いる値はいくつかありますが、今回は以下の3つの値を用います。

- 二乗平均平方根誤差（RMSE）と平均二乗誤差（MSE）
- 決定係数
- 残差プロット

をここで紹介します。


### 二乗平均平方根誤差（RMSE）と平均二乗誤差（MSE）

　誤差の二乗を平均して平方根をとったものです。RMSEは統計学の世界では最もよく使われる指標です。RMSEは以下の手順で求めています。

1. **予測した値と実際の値との誤差をとります**
2. **その誤差の値を2乗し、その総和をとります**
3. **総和をデータの件数で割って平均を取ります**
4. **最後に平方根を取ります**

　また、式で示すと以下のように示されます。理解する必要は必ずしもありません。

$$MSE = \frac{1}{n} \sum_{i=0}^{n-1} (y_i - \hat{y_i})^2$$
$$RMSE = \sqrt{\frac{1}{n} \sum_{i=0}^{n-1} (y_i - \hat{y_i})^2}$$
$$但し、 y_i=実際の値、\hat{y}=予測値、 n=データの総数 $$

　予測と正解の平均的なズレを表しており、大きな誤差を厳しく評価する特徴があります。
　実際にどのように求めていくのかみていきましょう。

```python
# 学習データに対する目的変数を予測
Y_train_pred = lr.predict(X_train)
# 検証データに対する目的変数を予測
Y_pred = lr.predict(X_test)
# 学習データを用いたときの二乗平均平方根誤差(RMSE)を算出
print('RMSE train data: ', sqrt(mean_squared_error(Y_train, Y_train_pred))) 
# 検証データを用いたときの二乗平均平方根誤差(RMSE)を算出
print('RMSE test data: ', sqrt(mean_squared_error(Y_test, Y_pred)))
# 学習データを用いたときの平均二乗誤差（MSE）を算出
print('MSE train data: ', mean_squared_error(Y_train, Y_train_pred)) 
# 検証データを用いたときの平均二乗誤差（MSE）を算出
print('MSE test data: ', mean_squared_error(Y_test, Y_pred))
```

　結果は以下のものになっています。これは場合によって異なる可能性がありますので気にしないでください。ここで、注目したいのは、学習データ、検証データそれぞれを用いたときの平均二乗誤差を比較すると、検証データを用いたときの誤差の方が大きいことです。このことは、構築した線形モデルは学習データにフィットしすぎている、すなわち**過学習**であることが示されています。

```python
RMSE train data:  6.043506135262846
RMSE test data:  6.807077593213252
MSE train data:  36.52396640695966
MSE test data:  46.336305360025925
```

### 決定係数

　決定係数とは、実際の値と予測値の絶対値の2乗を平均したものです。
　線形モデルの予測誤差を反映した指標であり、0~1の範囲の値をとり、1に近いほど説明変数が目的変数を説明していることになり、データに対して当てはまりの良いモデルであると言えます。

　また、式で示すと以下のように示されます。理解する必要は必ずしもありません。

$$ R^2 = 1 - \frac{\sum_{i=0}^{n-1} (y_i - \hat{y_i})^2}{\sum_{i=0}^{n-1} (y_i - \bar{y_i})^2} $$
$$但し、 y_i=実際の値、\hat{y}=予測値、 \bar{y}=平均値、n=データの総数 $$

　決定係数は、様々な算出方法がありますが、今回はLinearRegressionモデルのscoreメソッドで算出します。
　scoreの構文は以下の通りです。

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression
model.score(Xデータ, Yデータ)
```

です。では、ボストンの住宅価格のデータセットに対して実際に決定係数を求めていきましょう。


```python
print('R^2 \n Train :' , model.score(X_train, Y_train), '\n Test :',model.score(X_test, Y_test))
```

　結果は以下のものになっています。MSEと同様に場合によって異なる可能性がありますので気にしないでください。ここで、注目したいのは、学習データ、検証データそれぞれを用いたときの決定係数を比較すると、検証データを用いたときの決定係数の方が小さいことです。このことからも、構築した単回帰モデルには過学習が起こっている可能性があることがわかります。

```python
R^2 
 Train : 0.571031588576562 
 Test : 0.43095672846187605
```

### 残差プロット
　残差プロットは、目的変数の真値と予測値の差分(残差)の分布をグラフ化したものです。モデルが目的変数を完璧に予測できる場合は残差は0となる性質を持っています。また、予測精度の良いモデルの残差プロットは、0を中心にランダムにばらついたものになります。
　残差プロットに何かパターンが見られる場合は、そのモデルで説明しきれない情報があることが示唆されます。

　残差とは以下のような部分のeの部分のことです。

IMG(残差を説明する画像）

　残差が徐々に大きくあるいは小さくなっていたり、何かしらの傾向が見られる場合は要注意です。この場合、推定された回帰式が妥当ではない可能性があります。
　残差の評価は、縦軸の「0」に対して、残差がどのようにばらついているかをみてみましょう。残差が縦軸「0」に対して均一に分散している場合、大きな問題はありません。残差プロットの中で他とは異なり大きく外れている値が少数ある場合、該当する値は外れ値である可能性があるということです。

　それでは、ボストンの住宅価格のデータセットで残差を求めてみましょう。

　残差プロットは、目的変数の真値と予測値の差分(残差)の分布をグラフ化したものです。ですので、方針としては以下のようになります。

- ①(目的変数の実際の値) - (分析の結果得られた直線から出てきた予測値)を求めます
- 横軸に予測の目的変数をとって、縦軸に①の値をとってグラフ（散布図）にプロットします
- 学習データと検証データに分けてこれを行います


　実際に書いてみましょう！

```python
# 学習用、検証用それぞれで残差をプロット
plt.scatter(Y_train_pred, Y_train_pred - Y_train, c = 'blue', marker = 'o', label = 'Train Data')
plt.scatter(Y_pred, Y_pred - Y_test, c = 'lightgreen', marker = 's', label = 'Test Data')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
# 凡例を左上に表示
plt.legend(loc = 'upper left')
# y = 0に直線を引く
plt.hlines(y = 0, xmin = -10, xmax = 50, lw = 2, color = 'red')
plt.xlim([10, 50])
fig.show()
```

　表示されたでしょうか？このままでは、グラフが２つ重なって表示されてしまい見にくいと思います。ですので、バラバラに表示するために少しコードを変更しましょう。scikit-learnのsubplotsというメソッドを使います。

　基本的にsubplotsの規則的に複数表示させるためのルールは以下のようになっています。

```python
subplot(行数, 列数, プロット番号)
```

　行数☓列数でfigureが分割されます。プロット番号は1行目から右方向に番号が増えるようにっており、ちなみに今回使ったのは1行2列なのでプロット番号は横方向に1,2と増えていきます。

詳しくは公式サイトを見てみましょう。
[subplotsの公式サイト](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.subplots.html)

```python
# グラフを定義
plt.subplot(1,2,1)
plt.scatter(xdata, ydata)
plt.subplot(1,2,1)
# すでにあるグラフを右側に表示しよう
plt.subplot(1,2,1)
plt.plot(x, func, color="red")
plt.title("boston_housing")
plt.xlabel("LSTAT")
plt.ylabel("TARGET")
# 新しく作ったグラフを左側に表示しよう
plt.subplot(1,2,2)
plt.scatter(Y_train_pred, Y_train_pred - Y_train, c = 'blue', marker = 'o', label = 'Train Data')
plt.scatter(Y_pred, Y_pred - Y_test, c = 'lightgreen', marker = 's', label = 'Test Data')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
# 凡例を左上に表示
plt.legend(loc = 'upper left')
# y = 0に直線を引く
plt.hlines(y = 0, xmin = -10, xmax = 50, lw = 2, color = 'red')
plt.xlim([10, 50])
# グラフの表示
fig.show()
```

　すると、以下のようなグラフが表示されるはずです。

IMGグラフ

　今回のモデルに関しては、概ねランダムに分布しているようにも見えますが、学習用データを使った場合の残差に関してグラフの右下になにか直線的な傾向があり、モデルに取り切れていない情報がある可能性があルといことがわかります。

# 結果をまとめてみよう
　
### まとめ
　

**------------------------------**

精度を上げよう-重回帰分析編
# この章の目標
　先ほど単回帰分析を学びました。このモデルの性能を上げていくようにするにはどのようにしたらよいでしょうか？単回帰は１つの変数でしたが、複数の変数を用いる重回帰分析と呼ばれる違う分析手法を用いてみましょう。

1. scikit-learnの重回帰分析の手法を学習しよう

# 概要
　平たく言えば、重回帰分析とは、単回帰分析では目的変数に最も関連のある説明変数を使って直線を引いていましたが、重回帰分析では複数の説明変数を用いて直線を引くこと分析方法です。

# 重回帰分析とは

　重回帰分析は、複数の説明変数から目的変数を予測する回帰分析のことを言います。裏の理論は高校数学で理解できるものではないので一旦置いておいて、まず使えるようになりましょう。
　高校数学で理解できるものではないですが、一旦下に一般式を載せておきますので興味があったら調べてみてください！

$$y = \sum_{m=1}^{M}w_mx_m + b $$

## 用いるデータを確認しよう

　まず、用いるデータの確認をしましょう。データは**目的変数['target']**と**説明変数['LSTAT']**のままですね。ここになんのデータを追加しましょう？
　再びヒートマップをみてみましょう！

　IMG(ヒートマップ)

　RMがLSTATに比べてボストンの住宅価格に関連度が強いことが見れれますので、こちらの変数を追加してやってみたいと思います。どれでは、変数を追加する方法を学んでいきましょう。

## 重回帰分析を行おう

　重回帰分析を行うにあたって、注意点があります。先ほどのような住宅価格とLSTATの値を比較して、直線を引くというのは重回帰分析を行う上であまりしません。これはなぜでしょうか？
　<font color='red'>**重回帰分析は変数が2つとは限りませんよね。計算能力が許す限り、100変数に対しても、いくつの変数に対しても行うことが理論上はできます**</font>。ですので、グラフにプロットしきれないのです。ですので、先ほどの散布図と直線のコードはコメントアウトするか、散布図と直線のコード以外を別のコードブロックへコピーしてから進めましょう。
　では早速いきましょう。まず、reshapeする必要がなくなったので、reshapeを削除します。この理由は、行列を学んだらわかるので、もうちょっと先の数学まで我慢です。そして、先ほど見た**RM**を学習・検証データ双方から取り出していきましょう。

```python
   # 訓練データとテストデータの作成
   X_train = train[["RM", "LSTAT"]]
   Y_train = train["target"] #修正済み住宅価格中央値
   X_test = test[["RM", "LSTAT"]]
   Y_test = test["target"]
```

　はい、これもこれで終了です。実は、**LinearRegression()モデルは、重回帰分析も単回帰分析**も渡す変数の違いによって行うことができるモデルなのです。ですので、これだけでできてしまいます。
　性能を図る部分のコードも変数名さえ間違っていなければそのまま動きます！
　早速実行して性能をみましょう！


## 各指標をみてみよう

```python
#単回帰分析のとき
RMSE train data:  6.043506135262846
RMSE test data:  6.807077593213252
MSE train data:  36.52396640695966
MSE test data:  46.336305360025925
R^2 
 Train : 0.571031588576562 
 Test : 0.43095672846187605
# 重回帰分析のとき
RMSE train data:  5.365657134224422
RMSE test data:  6.114172522817781
MSE train data:  28.790276482053443
MSE test data:  37.38310563877995
R^2 
 Train : 0.6618625964841893 
 Test : 0.5409084827186418
```

TODO コメント


### 今回行った分析で得られた結果というのはどのようなものなのか？
　今回得られた式の意味を一回考えてみましょう。
　この式の意味を理解することは非常に重要です。式そのものを日本語に翻訳でき、相手に伝えることも人工知能エンジニアやデータサイエンティストの評価されるスキルのひとつです。それは、いくらデータ解析が上手くても、解析しただけでは意味がないからです。それでは、各変数の係数を求めていきましょう。
　下記のコードを実行してみましょう。

```python
pd.DataFrame({"name":X_train.columns,"coefficients":model.coef_})
```

　以下のように各変数と変数の係数が出てきたと思います。この変数の意味を考えてみましょう

||name  |coefficients|
|:--:|:--:|:--:|
|0 |    RM|      5.109068|
|1 | LSTAT|     -0.654949|

これは、
$$TARGET = 5.1RM -0.65LSTAT + 切片$$
という結果を導いています。この結果は、**部屋数の指標はプラスに働き、貧困人口割合の指標はマイナスに働いているということを示しています**。これはおおよそ大半の人の感覚と正しいのではないでしょうか？

### さらに精度をあげてみよう
　ここからさらに精度をあげてみたいと思います。次章ではまた異なる分析手法を用いてみたいと思います。

# まとめ

########################################################

**------------------------------**

精度を上げよう-多項式回帰編
# この章の目標
　ここまで、単回帰分析と重回帰分析を学んできました。このモデルは直線を引くという点で直感的でわかりやすいというメリットがあります。ですが、直線で全てが近似できるわけではありません。曲線の方が多くの場合に最適にデータに近似している線がかけるでしょう。その書き方を学んでいきましょう。

scikit-learnのPolynomialFeartu

1. 多項式回帰の手法を学習しよう

# 概要
　多項式回帰というものについて学習します。ここでは定義を軽く示しますが、高校数学の範囲を逸脱するものも含まれています。ですので、ここではあくまで使えるようになるのが目標です。理解するのはしばらく取っておきましょう。

# 多項式回帰分析とは

　回帰分析のような直線で世の中を全て近似できたら楽なことはありません。ですが、現実はもっと複雑だとみんな知っています。そんな複雑で1次関数(単・重回帰分析)で直線的な関係が、すなわちyとxに見られない時は多項式回帰が有効な手段です。
　多項式回帰では2次関数や3次関数のような、次数の高い曲線的な分布のデータの予測を行うことが出来ます。多項式回帰の一般式は以下に示されます。理解する必要はありません。
　ここでの**最優先目標は多項式回帰を使えることになること**です。理論は後からで構いません。

$$\displaystyle y=\sum_{i=0}^{n}\beta_ix^i$$
$$\beta = パラメータ｜n = 次数$$

## 用いるデータを確認しよう

　まず、用いるデータの確認をしましょう。データは**目的変数['target']**と**説明変数['LSTAT','RM']**を用いて多項式回帰を行っていきます。

　重回帰分析と同じ変数を用いるので、どのように変化するかもみてみましょう。

## 重回帰分析を行おう

　重回帰分析のコードを別のコードブロックへコピーしてから進めましょう。
　では早速いきましょう。以下のコードを書いてみましょう。

```python
  from sklearn.preprocessing import PolynomialFeatures
   #n=2で試してみます。
   # データの生成方法の指定
   poly_reg = PolynomialFeatures(degree = 2)
   # データをルールに基づいて生成
   X_train_poly = poly_reg.fit_transform(X_train)
   X_test_poly = poly_reg.fit_transform(X_test)

    # 以下全ての
    # X_train を X_train_poly
    # X_test を X_test_poly
    # へ変更
```


　はい、これもこれで終了です。実は、**LinearRegression()モデルは、PolynomialFeaturesと組み合わせることによって**多項式回帰分析を行うことができるモデルなのです。ですので、これだけでできてしまいます。
　また、これは2次の曲線で近似していますが、**degree**の値を3,4,5...と増やしていくと字数を上げることができます。しかしながら、予測モデルに、次数の増えたデータを学習させても過学習（フィットしすぎて）になり予測精度が悪くなる可能性があります。
　では、この2時の結果をみてみましょう。


## 各指標をみてみよう

```python
#単回帰分析のとき
RMSE train data:  6.043506135262846
RMSE test data:  6.807077593213252
MSE train data:  36.52396640695966
MSE test data:  46.336305360025925
R^2 
 Train : 0.571031588576562 
 Test : 0.43095672846187605
# 重回帰分析のとき
RMSE train data:  5.365657134224422
RMSE test data:  6.114172522817781
MSE train data:  28.790276482053443
MSE test data:  37.38310563877995
R^2 
 Train : 0.6618625964841893 
 Test : 0.5409084827186418
#多項式回帰分析のとき
RMSE train data:  4.319658995006362
RMSE test data:  5.30518349005234
MSE train data:  18.659453833139377
MSE test data:  28.14497186312392
R^2 
 Train : 0.7808475624020519 
 Test : 0.6543594328054229
```

　決定係数の値が１に近ずいてきており、多項式回帰分析でより現実世界に近いモデルが作れたと言えます。

### 精度の結果をより詳しくみていこう
　次章からは精度の結果をより深く掘り下げてその意味を考えてみたいと思います。

# まとめ




########################################################

**------------------------------**

精度の結果を比較してみよう
# この章の目標
　単回帰分析、重回帰分析、多項式分析の手法の性能を比較してみましょう。また、求めた直線、または曲線をグラフにしてみましょう。

全ての比較

1. 精度を比較して、図を求めてみんなに納得してもらおう

# 概要
　求めた直線、または曲線の図を取得する方法を学びましょう。また、モデルの正確性について詳しく検討していこう

# 精度の正確性をみていこう
　精度の正確性は今までは、決定係数、二乗誤差、平均二乗誤差でみてきました。
　ですが、例えば平均どれくらいの価格のズレがあるのか？や実際に比べて何パーセントくらいズレているのかといったほうがパッとみてわかりやすいと思いませんか？
　ですので、それらの指標をここでは加えて紹介します。

## 新しい精度の指標を学ぼう

　「実際に比べて何パーセントくらいズレているのか」は、平均絶対誤差率(MAPE)というのを用います。
　「平均どれくらいの価格のズレがあるのか？」は、平均絶対誤差(MAE)というものを使います。

### 平均絶対誤差率(MAPE)

　平均絶対誤差率というものは、

$$MAPE = \frac{100}{N} \sum_{i=1}^{N} \left|\frac{\hat y}{y_i} - 1\right|$$
$$N=データ数 | \hat{y}=実際のyの値 | y_i=測定されたyの値$$

で示される量のことです。すべての予測した点について絶対値を求めていきます。そしてそれらを合計し、予測そた地点の総数nで割ります。最後に100をかけることでパーセントにしています。非常にわかりやすい指標ですね。ですが、この指標には多くの問題点があり、全てに用いれるわけではないです。

　区分と比率が意味のある値に対してのみMAPEは意味を持ちます。たとえば、温度ような割合を計算することができないものには用いることができません。
　また、0がデータに存在する場合にもゼロで割ることになってしまうため、使えません。
　また誤差が小さすぎる場合に、MAPEがとても大きくなるという問題もあります。100を超えてしまうことも考えられます。
　ですので用いる時は注意が必要です。

　平均誤差率は、pythonでは以下のようにかけます。ここでは、Numpyというモジュールの平均値を求めるmeanというメソッドを使っています。
　（）内は、多項式回帰の場合を示しています。

```python
print("平均誤差率(test):", np.mean(abs(実際の値(Y_test) / モデルの予測値(Y_pred) - 1)))
```

### 平均絶対誤差(MAE)

　平均絶対誤差というものは、

$$MAE = \frac{1}{N} \sum_{i=1}^{N} \left| \hat{y} - y_i \right|$$
$$N=データ数 | \hat{y}=実際のyの値 | y_i=測定されたyの値$$

で示されます。すべての予測した点について絶対値を求めていきます。そしてそれらを合計し、予測そた地点の総数nで割ります。これが何に役立つのでしょうか？
　これは、そのモデルがどれだけズレているか目的変数の単位で表示させるようにすることができます。例えば、今回の例で言えば、いくら住宅価格が平均してズレているかということを示すことができます。

　平均絶対誤差は、pythonでは以下のようにかけます。ここでは、scikit-learnの平均絶対誤差を求めるmean_absolute_errorを使っています。
　（）内は、多項式回帰の場合を示しています。

```python
from sklearn.metrics import mean_absolute_error
print("MAE(test):", mean_absolute_error(Y_test, Y_pred))
```

### 各略語について

　MAEにR2にと略語が多く出てきて何が何だかわからないかと思うのでここで以下の表で整理します。わからなくなったらここに戻ってきましょう。

|略語|意味|日本語|
|:--:|:--:|:--:|
|RMSE|Root Mean Squared Error|二乗平均平方根誤差|
|MSE|Mean Squared Error|平均二乗誤差|
|$R^2$|決定係数|決定係数|
|MAE|Mean Absolute Error|平均絶対誤差|
|MAPE|Mean Absolute Percentage Error|平均絶対誤差率|

## 新しい指標を使って誤差を比較してみよう
　それでは、新しい2つの新しい指標を使って比較してみましょう。
　ここでは、コードを書くよりも、分析するのがメインですので、すでに完成しているコードを下記に用意しましたので、リンク先で実行してみてみましょう。

[Google Colabの誤差比較完成コード](https://colab.research.google.com/drive/1mCmAymmQmEB2cZ8L_JvTFkm5BfBO6-gi?usp=sharing)

|指標|単回帰分析|重回帰分析|多項式回帰分析|
|:--:|:--:|:--:|:--:|
|RMSE(train)|6.04|5.37|4.32|
|RMSE(test)|6.81|6.11|5.31|
|MSE(train)|36.52|28.79|18.66|
|MSE(test)|46.34|37.38|28.14|
|R2(train)|0.57|0.66|0.78|
|R2(test)|0.43|0.54|0.65|
|MAE(test)|4.86|4.14|3.34|
|MAPE(test)|0.31|0.25|0.16|

　以上のような表が見れたと思います。これから見ると、
　RMSEは多項式回帰分析が最も学習と検証の乖離が大きく、これは外れ値をうまく学習しきれていないことを表しています。だから決して使えないというわけではなく、そのデータを利用する場合にこれはよります。例えば外れ値に注目したい場合に、これは活用することはできませんが、中央値のような値に注目したい場合には有用であると言えます。
　R2からは、1に近ずいておりより当てはまりが良くなっています。しかし、モデルの当てはまりすぎ「too good to be true」すなわち、「そのモデルの精度が高過ぎるんじゃないか」という原因は「実は汎化性能を見てませんでした」といことを示していることもありますので汎用性に注意しましょう。汎用性の指標には、時と場合によりますので、場合によってはこれらの指標以外のものを使わなければならない可能性もあります。
　MAEからは多項式回帰分析は$3340のズレと単回帰分析に比べ約40%、重回帰分析に比べ約20%よく住宅価格を予測できており、精度向上がみられます。
　MAPEからは多項式回帰分析は標準から16%のズレと単回帰分析に比べるとよく近似できていることがわかります。

# それぞれのグラフを書こう
　それでは、残差プロットを並べてみてみましょう。
　再びここでは、コードを書くよりも、分析するのがメインですので、すでに完成しているコードを下記に用意しましたので、リンク先で実行してみてみましょう。

[Google Colabの残差プロット完成コード](https://colab.research.google.com/drive/1jkTnNeNGKxFu8DWX-kCYUifC-kwIMSEm?usp=sharing)

IMG(残渣プロット)

　残差プロットは、目的変数の真値と予測値の差分(残差)の分布をグラフ化したものです。モデルが目的変数を完璧に予測できる場合は残差は0となる性質を持っています。また、予測精度の良いモデルの残差プロットは、0を中心にランダムにばらついたものになります。残差プロットに何かパターンが見られる場合は、そのモデルで説明しきれない情報があるということでしたね。
　以上の3つのグラフはどうでしょうか？単回帰分析、重回帰分析、多項式回帰分析の順で0を中心にランダムにばらついたものになっていないでしょうか？
　しかしながら、多項式回帰分析になるにつれて、直線的な傾向強くなり、モデルに取り切れていない情報の影響が大きくなっているといことがわかります。
　これは、先ほど見たRMSEの結果と一致していますね。

**------------------------------**

自分でデータ解析してみよう
# この章の目標
　自分でデータ解析するのに必要な知識を整理しましょう。

1. 自分でデータ解析するのに必要な知識を整理する

# 概要
　自分でデータ解析する課題に取り組む前に知識を整理しておきましょう。

# はじめに
　機械学習の基礎となる考え方をざっと学習しました。いかがでしたでしょうか？
　かなりのボリュームがあったのではないでしょうか？また、大半の方は人工知能、機械学習のイメージが変わったのではないでしょうか？
　では自分で機械学習をする前に少しだけプラスαの知識をお伝えします。早速やりたい気持ちでうずうずしている方は、こちらは飛ばしても構いませんし、困ったら戻ってくるでも構いません。

# どこまで正確性を求めるか？
　どれくらいなら過学習か？そのモデルはどれくらいの精度ならOKなのか？疑問ではないでしょうか？
　これにお答えしたいと思います。
　その答えは、何%が出たらOKというような値は存在しません。<font color='red'>**「価値があるか」**</font>そこが判断基準です。

　例えばバスの運転をする乗務員にバス停まで時間通り到着するための速度を予測し表示する人工知能を開発したとします。しかし、この人工知能にしたがっても時間通りに到着しない確率が10%発生してしまっています。ですが、人工知能を使わない場合は時間通りに到着しない確率が20%発生していました。この10%の差が減り、顧客満足度は増加し利用者数が増えるかもしれません。その観点ではこのAIは有用です。

　また、金型を作っている工場で、機械を調整する人工知能を導入しました。ですが、ベテランの職人が機械の調整をするより少し間違いが多いです。このAIは無用でしょうか？
　そんなことはありません。なぜなら、その人しか調整できないため、その人が病気や怪我、退職した際にも少し劣るかもしれないが今まで通りの運用が維持することができるため人的リスクを減らせるという観点では**このベテラン職人よりちょっとおバカなAIは有用**です。

　世の中は、人工知能でピッタリ予測できることなどありません。ですが、その精度が期待する通りでないから無用だ！と決めつけるのではなく、取り組み方次第では価値を生み出すこともあります。実際に人工知能を導入する、開発する立場になったら、<font color='red'>**どのような目的でそれを導入するのか？**</font>という観点が、どの程度正確性を求めるかということ考えてみましょう。

# 外部ファイルをGoogle Colabで読み込む方法
　ここでは、外部のファイルをGoogle Colabで読み込んで解析する方法を紹介します。
　実は、Google Colabがすでに詳しい方法を公開してくれていますし、これ以上にわかりやすいものはないのでGoogle Colab先生の肩にこのカリキュラムも乗ろうと思います。

[外部ソースからのデータの読み込みと保存に関するレシピ](https://colab.research.google.com/notebooks/io.ipynb?hl=ja#scrollTo=c2W5A2px3doP)

# 自分でどんなデータを機械学習させたらいいの？

　いきなり自由にやっていいですよと言われても困るかと思います。ですので、以下に方針や例を示しますのでこれに乗ってみても構いません。

### scikit-learnのトイデータをいじる

　冒頭でも紹介しましたが、scikit-learnにはたくさんのデータセットが用意されています。この中で回帰分析に適しているのは以下の表の一番最後の列に回帰と書かれていものです。

|データセット名|呼び出し方||
|:--:|:--:|:--:|
|ボストンの住宅価格|load_boston()|回帰|
|アイリス（アヤメ）の種類|load_iris()|分類|
|糖尿病の進行状況|load_diabetes()|回帰|
|手書き文字（数字）|load_digits|分類|
|生理学的測定結果と運動測定結果|load_linnerud()|回帰|
|ワインの種類|load_wine()|分類|
|がんの診断結果|load_breast_cancer()|分類|

　もちろん分類を試したくなったら試しても構いません。また、回帰のデータで同じようにデータ解析をしても構いません。

### ボストンの住宅価格のデータセットをいじる

　ボストンの住宅価格のデータセットはまだまだいじる余地が実はあります。例えば、

- 多項式回帰分析で次元をあげてみる
- 重回帰分析の項目を変えてみる
- 多項式回帰分析でも項目を変えてみる
- 項目を変えてみた結果を比較する

などです。こちらを試してみてもいいでしょう。

### Real world datasets

　scikit-learnにはToy datasets以外にも、ダウンロードが必要ですが、「Real world datasets」データセットというデータ件数が多いデータセットが用意されています。こちらはより現実に近いデータとなっていますのでこちらで同様の機械学習を試してみるのもいいでしょう。

### 全くの別のデータを試す

　全く別のデータを上記の方法でGoogle Colabに読み取り機械学習しても構いません。

### 他の分析方法、回帰方法について考える

　ボストンの住宅価格のデータセットには他にもサポートベクター回帰、ランダムフォーレスト、アンサンブル学習ができるので調べて試してみても構いません。

# 最後に

　最後の発表会ですが、あくまで学んだことを発表する場です。完璧である必要はありませんし、ここまでおわらなくても構いません。気軽に好きなようにやってみましょう！
　また、一人では難しいと思う場合は、ペアになっても構いません！

### Good Luck!

**------------------------------**

<!-- そのほかの分析手法
# この章の目標
　そのほかの分析手法について表面だけですがさらっと紹介します。いずれも今の高校の範囲の知識では理解することは不可能でしょう。ですが、大学または社会ではたくさんのデータ解析の手法が皆さんに使われるのを待っています。
　この章を読んで興味が湧いたらどんどん先に進んで勉強していきましょう。全て前章までのコードを書き換える部分だけ記載しています。

ここにそのほかの分析方法

1. そのほかの分析手法について学習する

# 概要
  データサイエンティスト・AIエンジニアへの道標です。

# ニューラルネットワーク

```python
#正規化
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler()  
scaler.fit(X_train)  
X_train = scaler.transform(X_train)  
X_test = scaler.transform(X_test)
# モデル作成
from sklearn.neural_network import MLPRegressor
neural_network = MLPRegressor(hidden_layer_sizes=(50)) 
neural_network.fit(X_train,Y_train) 
```

# サポートベクター回帰

```python
# モデル作成
import statsmodels.api as sm
from sklearn.svm import SVR
svr = SVR(kernel='rbf', C=1e3, gamma='scale')
svr.fit(X_train,Y_train) 
```

# ランダムフォーレスト

```python
import xgboost as xgb
random_forest = xgb.XGBRegressor(objective ='reg:squarederror')
random_forest.fit(X_train,Y_train)
```

# アンサンブル学習

```python
import xgboost as xgb
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import VotingRegressor
ensemble1 = GradientBoostingRegressor(random_state=1, n_estimators=10)
ensemble2 = RandomForestRegressor(random_state=1, n_estimators=10)
ensemble3 = LinearRegression(normalize=True)
ereg = VotingRegressor(estimators=[('xgb', ensemble1), ('rf', ensemble2), ('lr', ensemble)])
ereg = ereg.fit(X_train, Y_train)
ereg.estimators
``` -->
**------------------------------**